{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperbatch Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this sets up the Matplotlib interactive windows:\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hyperbatch_efficinecy` sums the batch size calculations from the modified samplers\n",
    "and returns `total_samples_for_regular_batch / total_samples_for_hyperbatch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that the schedule:\n",
    "# - is an Iterator[tuple[current_size, appended_size | None, int]]\n",
    "# - list(map(lambda x: x[-1], schedule)) == list(range(num_steps - 1))\n",
    "# - map(lambda x: x[1], output) == range(num_steps - 1)\n",
    "def batch_doubling_schedule_validate_and_final_size(schedule):\n",
    "    current_index = 0\n",
    "    current_size = 1\n",
    "    for expected_current_size, appended_size, i in schedule:\n",
    "        assert current_index == i, f\"current_index has unexpected value: {current_index} != {i}\"\n",
    "        current_index += 1\n",
    "        if appended_size is None:\n",
    "            current_size *= 2\n",
    "        else:\n",
    "            current_size += appended_size\n",
    "        assert expected_current_size == current_size, f\"expected_current_size != current_size: {expected_current_size} != {current_size}\"\n",
    "\n",
    "    return current_size\n",
    "\n",
    "# The doubling pattern has shape:\n",
    "# d steps, 2x, d steps, 2x, .., 2x, d steps, leftover/2x, d steps\n",
    "# K 2xâ€™s/leftovers\n",
    "# K*d + d = (K+1)*d = steps\n",
    "# steps/(K + 1) = d\n",
    "# zs = [0] * d\n",
    "# If leftovers\n",
    "# (zs + [None]) * (K-1) + zs + leftovers + zs\n",
    "# Else\n",
    "# (zs + [None]) * K + zs\n",
    "# Note: this function asserts that\n",
    "# - batch_doubling_schedule_validate_and_final_size's assertions holds\n",
    "# - batch_doubling_schedule_validate_and_final_size(output) == batch_size\n",
    "def batch_doubling_schedule(batch_size, num_steps):\n",
    "    batch_size_log2 = batch_size.bit_length()\n",
    "    batch_size_leftover = None\n",
    "    if 2 ** batch_size_log2 != batch_size:\n",
    "        batch_size_log2 -= 1\n",
    "        batch_size_leftover = batch_size - 2 ** batch_size_log2\n",
    "\n",
    "    if num_steps <= batch_size_log2:\n",
    "        print(f\"The number of steps must be greater than log2(batch_size) to use a Hyperbatch scheduler: disabling Hyperbatch functionality.\")\n",
    "        schedule = list(map(lambda i: (batch_size, 0, i), range(num_steps)))\n",
    "        assert len(schedule) == num_steps, f\"len(schedule) != num_steps: {len(schedule)} != {num_steps}: {batch_size_log2} {schedule}\"\n",
    "        return schedule\n",
    "\n",
    "    substep_length = num_steps // (batch_size_log2 + 1)\n",
    "    substeps = [0] * (substep_length - 1)\n",
    "    schedule = (substeps + [None]) * batch_size_log2 + substeps + [batch_size_leftover]\n",
    "    schedule += [0] * (num_steps - len(schedule))\n",
    "    current_batch_size = 1\n",
    "    def add_current_batch_size(i_appended_size):\n",
    "        nonlocal current_batch_size\n",
    "        i, appended_size = i_appended_size\n",
    "        if appended_size is None:\n",
    "            current_batch_size *= 2\n",
    "        else:\n",
    "            current_batch_size += appended_size\n",
    "        return (current_batch_size, appended_size, i)\n",
    "\n",
    "    schedule = list(map(add_current_batch_size, enumerate(schedule)))\n",
    "    final_size = batch_doubling_schedule_validate_and_final_size(schedule)\n",
    "    assert batch_size == final_size, f\"batch_size not equal to current_size: {batch_size} != {final_size} \\n {schedule}\"\n",
    "    assert len(schedule) == num_steps, f\"len(schedule) != num_steps: {len(schedule)} != {num_steps}: {schedule}\"\n",
    "    return schedule\n",
    "\n",
    "print(batch_doubling_schedule(8, 7))\n",
    "\n",
    "for i in range(1, 10):\n",
    "    for j in range(1, 10):\n",
    "        batch_doubling_schedule(i, j)\n",
    "\n",
    "def hyperbatch_efficiency(batch_size, num_steps):\n",
    "    total_image_steps = 0\n",
    "    for current_batch_size, appended_size, i in batch_doubling_schedule(batch_size, num_steps):\n",
    "        total_image_steps += current_batch_size\n",
    "    return float(batch_size * num_steps) / total_image_steps\n",
    "\n",
    "plt.close('all')\n",
    "max_batch_size = 128\n",
    "batch_size_range = range(1, max_batch_size)\n",
    "fig, axs = plt.subplots(4, 1, sharex=True, constrained_layout=True)\n",
    "fig.set_size_inches(9.25, 5.25)\n",
    "fig.suptitle('Hyperbatch efficiency at different step counts')\n",
    "for i, ax in enumerate(axs):\n",
    "    num_steps = 10 * (i + 2)\n",
    "    ax.set_title(f\"{num_steps} steps\", loc='left')\n",
    "    ax.plot(batch_size_range, list(map(lambda x: hyperbatch_efficiency(x, num_steps), batch_size_range)))\n",
    "    ax.set_xlabel('Batch size')\n",
    "    # xticks = range(1, max_batch_size, 10)\n",
    "    # ax.set_xticks(xticks)\n",
    "    ax.set_ylabel('Speedup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
